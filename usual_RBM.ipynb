{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQ8ANSix0LB5"
      },
      "source": [
        "### The below tensorflow code is compatible with Tensorflow v1.x and we're using tensorflow v2.x which would throw multiple exceptions... Hence, we'll use tf.compat.v1.v1_method_name() after disabling the eager execution whenever needed  \n",
        "(will be needed in very few places so okay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3xpHcc1JFbm"
      },
      "outputs": [],
      "source": [
        "# !pip install tensorflow_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkZYthpeIh-k"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf #Deep learning library\n",
        "import numpy as np #Matrix algebra library\n",
        "\n",
        "# the below method throws deprecated warnings so will go with the keras mnist dataset instead...\n",
        "# from tensorflow.examples.tutorials.mnist import input_data #Import training data\n",
        "\n",
        "# imported the below dataset earlier but don't yet know how to divide it into training and test splits...\n",
        "# import tensorflow_datasets\n",
        "# mnist = tensorflow_datasets.load('mnist', split='train', shuffle_files=True)\n",
        "# mnist = mnist.shuffle(1024).repeat().batch(32)  # refer to https://www.gcptutorials.com/post/load-mnist-data-with-tensorflow-datsets\n",
        "# for example in mnist.take(1):\n",
        "#     image, label = example['image'], example['label']\n",
        "#     print(image.shape)\n",
        "#     print(label)\n",
        "\n",
        "# for i in image:\n",
        "#     plt.imshow(tf.squeeze(i))\n",
        "#     plt.show()\n",
        "\n",
        "import pandas as pd #Database management library\n",
        "import matplotlib.pyplot as plt #Visualization library \n",
        "%matplotlib inline  \n",
        "from sklearn.svm import LinearSVC #For linear image classification\n",
        "from sklearn.model_selection import GridSearchCV #For hyperparameter optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVVfseTTwAYm",
        "outputId": "b8b56a9b-f269-4420-ba27-e110addfaad5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n"
          ]
        }
      ],
      "source": [
        "(trX1, trY1), (teX1, teY1) = tf.keras.datasets.mnist.load_data()\n",
        "print(trX1.shape)\n",
        "print(trY1.shape)\n",
        "print(teX1.shape)\n",
        "print(teY1.shape)\n",
        "trX_mnist=trX1.reshape(-1,trX1.shape[1]*trX1.shape[2]).astype(np.float32)\n",
        "teX_mnist=trX1.reshape(-1,teX1.shape[1]*teX1.shape[2]).astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXMBNj555WyN",
        "outputId": "c44729ab-d162-4ba9-f6ad-5170e6bf7b15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiOvi0TNzkDb"
      },
      "outputs": [],
      "source": [
        "ds, info = tfds.load('mnist', split='train', with_info=True)\n",
        "\n",
        "mnist_df = (lambda n=60000: tfds.as_dataframe(ds.take(n), info))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkMPHZKi2eAb",
        "outputId": "00bfe133-82bf-4c36-99be-8ac67df29d9d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tfds.core.DatasetInfo(\n",
              "    name='mnist',\n",
              "    full_name='mnist/3.0.1',\n",
              "    description=\"\"\"\n",
              "    The MNIST database of handwritten digits.\n",
              "    \"\"\",\n",
              "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
              "    data_path='~/tensorflow_datasets/mnist/3.0.1',\n",
              "    file_format=tfrecord,\n",
              "    download_size=11.06 MiB,\n",
              "    dataset_size=21.00 MiB,\n",
              "    features=FeaturesDict({\n",
              "        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
              "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
              "    }),\n",
              "    supervised_keys=('image', 'label'),\n",
              "    disable_shuffling=False,\n",
              "    splits={\n",
              "        'test': <SplitInfo num_examples=10000, num_shards=1>,\n",
              "        'train': <SplitInfo num_examples=60000, num_shards=1>,\n",
              "    },\n",
              "    citation=\"\"\"@article{lecun2010mnist,\n",
              "      title={MNIST handwritten digit database},\n",
              "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
              "      journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n",
              "      volume={2},\n",
              "      year={2010}\n",
              "    }\"\"\",\n",
              ")"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TanWp6yL2acr"
      },
      "outputs": [],
      "source": [
        "uni_labels = mnist_df().label.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oc4LnYne4T68",
        "outputId": "e98d2394-d2a7-49c9-f912-68c0477ce4ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([4, 1, 0, 7, 8, 2, 6, 3, 9, 5])"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "uni_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDqBHSpL5ywM"
      },
      "outputs": [],
      "source": [
        "# mnist_df(4)[mnist_df(4)['label'] == 4].to_csv(f\"/content/drive/MyDrive/RestrictedBoltzmannMachines/4_df.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZLcJDzu0nbz",
        "outputId": "3e5788b3-1598-46c2-a3dd-a322dd877db8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label_4.csv is saved!\n",
            "label_1.csv is saved!\n",
            "label_0.csv is saved!\n",
            "label_7.csv is saved!\n",
            "label_8.csv is saved!\n",
            "label_2.csv is saved!\n",
            "label_6.csv is saved!\n",
            "label_3.csv is saved!\n",
            "label_9.csv is saved!\n",
            "label_5.csv is saved!\n"
          ]
        }
      ],
      "source": [
        "for label in uni_labels:\n",
        "  mnist_df()[mnist_df()['label'] == label].to_csv(f\"/content/drive/MyDrive/RestrictedBoltzmannMachines/label_{label}.csv\")\n",
        "  print(f\"label_{label}.csv is saved!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_Kpr3iBJvG6",
        "outputId": "135a350e-c446-4455-81dd-4b11458a2ec1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(trX2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0zUm8qXlsNs",
        "outputId": "c00930f6-5752-4798-879d-b032bf758d57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n",
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n"
          ]
        }
      ],
      "source": [
        "(trX3, try3), (teX3, teY3) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "print(trX3.shape)\n",
        "print(try3.shape)\n",
        "print(teX3.shape)\n",
        "print(teY3.shape)\n",
        "trX_fashion_mnist=trX3.reshape(-1,trX3.shape[1]*trX3.shape[2]).astype(np.float32)\n",
        "teX_fashion_mnist=trX3.reshape(-1,teX3.shape[1]*teX3.shape[2]).astype(np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vs_nFEXuXb1"
      },
      "source": [
        "# Referred to [JosephGatto's GitHub Repo](https://github.com/JosephGatto/Simplified-Restricted-Boltzmann-Machines/blob/master/Restricted_Boltzmann_Machine.ipynb) for building a RBM from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUNBAqZwGKhK"
      },
      "source": [
        "# Restricted Boltzmann Machine Class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vafEAYHLS3ck"
      },
      "source": [
        "### In the train() method of the class RBM, we see the use of placeholder in tensorflow 2.x version (current version) but we might face errors because tensorflow 2.x doesn't have the placeholders property (only tensorflow version 1.x had it), so we need to migrate the tensorflow 1.x code into tensorflow 2.x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iweDioC4yMxh"
      },
      "outputs": [],
      "source": [
        "# disabling the eager execution\n",
        "tf.compat.v1.disable_eager_execution()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RupX5Mcy1iJO"
      },
      "outputs": [],
      "source": [
        "class RBM(object):\n",
        "    def __init__(self, input_size, output_size, learning_rate, batch_size):\n",
        "        self.input_size = input_size #Size of the input layer\n",
        "        self.output_size = output_size #Size of the hidden layer\n",
        "        self.epochs = 2 #How many times we will update the weights \n",
        "        self.learning_rate = learning_rate #How big of a weight update we will perform \n",
        "        self.batch_size = batch_size #How many images will we \"feature engineer\" at at time \n",
        "        self.new_input_layer = None #Initalize new input layer variable for k-step contrastive divergence \n",
        "        self.new_hidden_layer = None\n",
        "        self.new_test_hidden_layer = None\n",
        "        \n",
        "        #Here we initialize the weights and biases of our RBM\n",
        "        #If you are wondering, the 0 ixzs the mean of the distribution we are getting our random weights from. \n",
        "        #The .01 is the standard deviation.\n",
        "        self.w = np.random.normal(0,.01,[input_size,output_size]) #weights\n",
        "        self.hb = np.random.normal(0,.01,[output_size]) #hidden layer bias\n",
        "        self.vb = np.random.normal(0,.01,[input_size]) #input layer bias (sometimes called visible layer)\n",
        "        \n",
        "        \n",
        "        #Calculates the sigmoid probabilities of input * weights + bias\n",
        "        #Here we multiply the input layer by the weights and add the bias\n",
        "        #This is the phase that creates the hidden layer\n",
        "    def prob_h_given_v(self, visible, w, hb):\n",
        "        return tf.nn.sigmoid(tf.matmul(visible, w) + hb)\n",
        "        \n",
        "        #Calculates the sigmoid probabilities of input * weights + bias\n",
        "        #Here we multiply the hidden layer by the weights and add the input layer bias\n",
        "        #This is the reconstruction phase that recreates the original image from the hidden layer\n",
        "    def prob_v_given_h(self, hidden, w, vb):\n",
        "        return tf.nn.sigmoid(tf.matmul(hidden, tf.transpose(w)) + vb)\n",
        "    \n",
        "    #Returns new layer binary values\n",
        "    #This function returns a 0 or 1 based on the sign of the probabilities passed to it\n",
        "    #Our RBM will be utilizing binary features to represent the images\n",
        "    #This function just converts the features we have learned into a binary representation \n",
        "    def sample_prob(self, probs):\n",
        "        return tf.nn.relu(tf.sign(probs - tf.random.uniform(tf.shape(probs))))  # tf.random_uniform -> tf.random.uniform\n",
        "    \n",
        "    def train(self, X,extract_weights=None):\n",
        "        #Initalize placeholder values for graph\n",
        "        #If this looks strange to you, then you have not used Tensorflow before\n",
        "        _w = tf.compat.v1.placeholder(tf.float32, shape = [self.input_size, self.output_size])\n",
        "        _vb = tf.compat.v1.placeholder(tf.float32, shape = [self.input_size])\n",
        "        _hb = tf.compat.v1.placeholder(tf.float32, shape = [self.output_size])\n",
        "        \n",
        "        \n",
        "        #initalize previous variables\n",
        "        #we will be saving the weights of the previous and current iterations\n",
        "        pre_w = np.random.normal(0,.01, size = [self.input_size,self.output_size])\n",
        "        pre_vb = np.random.normal(0, .01, size = [self.input_size])\n",
        "        pre_hb = np.random.normal(0, .01, size = [self.output_size])\n",
        "        \n",
        "        #initalize current variables\n",
        "        #we will be saving the weights of the previous and current iterations\n",
        "        cur_w = np.random.normal(0, .01, size = [self.input_size,self.output_size])\n",
        "        cur_vb = np.random.normal(0, .01, size = [self.input_size])\n",
        "        cur_hb = np.random.normal(0, .01, size = [self.output_size])\n",
        "               \n",
        "        #Plaecholder variable for input layer\n",
        "        v0 = tf.compat.v1.placeholder(tf.float32, shape = [None, self.input_size])\n",
        "        \n",
        "        #pass probabilities of input * w + b into sample prob to get binary values of hidden layer\n",
        "        h0 = self.sample_prob(self.prob_h_given_v(v0, _w, _hb ))\n",
        "        \n",
        "        #pass probabilities of new hidden unit * w + b into sample prob to get new reconstruction\n",
        "        v1 = self.sample_prob(self.prob_v_given_h(h0, _w, _vb))\n",
        "        \n",
        "        #Just get the probailities of the next hidden layer. We wont need the binary values. \n",
        "        #The probabilities here help calculate the gradients during back prop \n",
        "        h1 = self.prob_h_given_v(v1, _w, _hb)\n",
        "        \n",
        "        \n",
        "        #Contrastive Divergence\n",
        "        positive_grad = tf.matmul(tf.transpose(v0), h0) #input' * hidden0\n",
        "        negative_grad = tf.matmul(tf.transpose(v1), h1) #reconstruction' * hidden1\n",
        "        #(pos_grad - neg_grad) / total number of input samples \n",
        "        CD = (positive_grad - negative_grad) / tf.cast(tf.shape(v0)[0], tf.float32)   # tf.to_float(tf.shape(v0)[0])  -> tf.cast(tf.shape(v0)[0], tf.float32)\n",
        "        \n",
        "        #This is just the definition of contrastive divergence \n",
        "        update_w = _w + self.learning_rate * CD\n",
        "        update_vb = _vb + tf.reduce_mean(v0 - v1, 0)\n",
        "        update_hb = _hb + tf.reduce_mean(h0 - h1, 0)\n",
        "        \n",
        "        #MSE - This is our error function\n",
        "        err = tf.reduce_mean(tf.square(v0 - v1))\n",
        "        \n",
        "        #Will hold new visible layer.\n",
        "        errors = []\n",
        "        hidden_units = []\n",
        "        reconstruction = []\n",
        "        \n",
        "        test_hidden_units = []\n",
        "        test_reconstruction=[]\n",
        "        \n",
        "        \n",
        "        #The next four lines of code intitalize our Tensorflow graph and create mini batches\n",
        "        #The mini batch code is from cognitive class. I love the way they did this. Just giving credit! \n",
        "        with tf.compat.v1.Session() as sess:  # now it's compatible with v1\n",
        "            sess.run(tf.compat.v1.global_variables_initializer())\n",
        "            for epoch in range(self.epochs):\n",
        "                for start, end in zip(range(0, len(X), self.batch_size), range(self.batch_size, len(X), self.batch_size)):\n",
        "                    batch = X[start:end] #Mini batch of images taken from training data\n",
        "                    \n",
        "                    #Feed in batch, previous weights/bias, update weights and store them in current weights\n",
        "                    cur_w = sess.run(update_w, feed_dict = {v0:batch, _w:pre_w , _vb:pre_vb, _hb:pre_hb})\n",
        "                    cur_hb = sess.run(update_hb, feed_dict = {v0:batch, _w:pre_w , _vb:pre_vb, _hb:pre_hb})\n",
        "                    cur_vb = sess.run(update_vb, feed_dict = {v0:batch, _w:pre_w , _vb:pre_vb, _hb:pre_hb})\n",
        "                    \n",
        "                    #Save weights \n",
        "                    pre_w = cur_w\n",
        "                    pre_hb = cur_hb\n",
        "                    pre_vb = cur_vb\n",
        "                \n",
        "                #At the end of each iteration, the reconstructed images are stored and the error is outputted \n",
        "                reconstruction.append(sess.run(v1, feed_dict={v0: X, _w: cur_w, _vb: cur_vb, _hb: cur_hb}))        \n",
        "                print(f\"Learning Rate: {self.learning_rate}:  Batch Size: {self.batch_size}:  Hidden Layers: {self.output_size}: Epoch: {(epoch+1)}: Error: {sess.run(err, feed_dict={v0: X, _w: cur_w, _vb: cur_vb, _hb: cur_hb})}:\")\n",
        "            \n",
        "            #Store final reconstruction in RBM object\n",
        "            self.new_input_layer = reconstruction[-1]\n",
        "            \n",
        "            #Store weights in RBM object\n",
        "            self.w = pre_w\n",
        "            self.hb = pre_hb\n",
        "            self.vb = pre_vb\n",
        "\n",
        "            if extract_weights == 1:\n",
        "                return (self.w, self.hb, self.vb)\n",
        "    \n",
        "    #This is used for Contrastive Divergence.\n",
        "    #This function makes the reconstruction your new input layer. \n",
        "    def rbm_output(self, X):\n",
        "        input_x = tf.constant(X)\n",
        "        _w = tf.constant(self.w,dtype=tf.float32)\n",
        "        _hb = tf.constant(self.hb,dtype=tf.float32)\n",
        "        _vb = tf.constant(self.vb,dtype=tf.float32)\n",
        "\n",
        "        out = tf.nn.sigmoid(tf.matmul(input_x, _w) + _hb)\n",
        "        with tf.compat.v1.Session() as sess:\n",
        "            sess.run(tf.compat.v1.global_variables_initializer())\n",
        "            return sess.run(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdYbZObbC8or"
      },
      "outputs": [],
      "source": [
        "class train_cdk:\n",
        "  def __init__(self,k,input_size, output_size, learning_rate,\n",
        "               batch_size,weights=None):\n",
        "    self.k=k\n",
        "    self.cdk_train_input=None\n",
        "    self.cdk_test_input=None\n",
        "    self.cdk_train_hidden = None #Variable to store hidden layer for training data\n",
        "    self.cdk_test_hidden = None  #Variable to store hidden layer for testing data\n",
        "    self.batch_size=batch_size\n",
        "  \n",
        "    self.rbm = RBM(input_size, output_size, learning_rate, batch_size)\n",
        "    self.weights=weights\n",
        "  def as_w(self):\n",
        "      self.rbm.w=self.weights\n",
        "  def ex_w(self,trX):\n",
        "    self.cdk_train_input = trX\n",
        "    return self.rbm.train(self.cdk_train_input,1)\n",
        "  def train(self,trX,teX):   \n",
        "    self.cdk_train_input = trX #Training data\n",
        "    self.cdk_test_input = teX  #Testing data \n",
        "    #Loop for contrastive divergence\n",
        "    for i in range(self.k):\n",
        "        print(f'CD: {int(i+1)}')\n",
        "        self.rbm.train(self.cdk_train_input) #Using reconstruction as input layer for CD\n",
        "        self.cdk_train_input = self.rbm.new_input_layer\n",
        "        self.cdk_train_hidden = self.rbm.rbm_output(self.cdk_train_input)\n",
        "        self.cdk_test_hidden = self.rbm.rbm_output(self.cdk_test_input)\n",
        "    return [self.cdk_train_hidden, self.cdk_test_hidden, self.cdk_train_input]\n",
        "  def get_weights(self):\n",
        "    return self.rbm.w\n",
        "  def update_weights(self,weight):\n",
        "    self.rbm.w=weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R617PMAmdteM"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "def compress(weights,r=0):\n",
        "  nComp=100\n",
        "  mu = np.mean(weights, axis=0)\n",
        "  pca=PCA(n_components=nComp).fit(weights)\n",
        "  compressed=pca.transform(weights)\n",
        "  if r==1:\n",
        "    return pca.inverse_transform(compressed)\n",
        "  return compressed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "ml3mGLpL49ig",
        "outputId": "17d76d09-d831-41c1-f8af-8c49eb08b0c1"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-b6fa981c9b1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrY1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
          ]
        }
      ],
      "source": [
        "trY1.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jwEHeCa536Gc",
        "outputId": "21739cbd-536b-495f-e92c-01ae9966a7fe"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-11ffd9d74552>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0mD_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m \u001b[0mD_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_real\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mD_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0mG_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'tf'"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import os\n",
        "#from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "\n",
        "#mnist = input_data.read_data_sets('../../MNIST_data', one_hot=True)\n",
        "mb_size = 32\n",
        "z_dim = 10\n",
        "X_dim = trX1.shape[1]\n",
        "y_dim = trY1.shape[0]\n",
        "h_dim = 128\n",
        "c = 0\n",
        "lr = 1e-3\n",
        "\n",
        "\n",
        "def plot(samples):\n",
        "    fig = plt.figure(figsize=(4, 4))\n",
        "    gs = gridspec.GridSpec(4, 4)\n",
        "    gs.update(wspace=0.05, hspace=0.05)\n",
        "\n",
        "    for i, sample in enumerate(samples):\n",
        "        ax = plt.subplot(gs[i])\n",
        "        plt.axis('off')\n",
        "        ax.set_xticklabels([])\n",
        "        ax.set_yticklabels([])\n",
        "        ax.set_aspect('equal')\n",
        "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
        "\n",
        "    return fig\n",
        "\n",
        "\n",
        "def xavier_init(size):\n",
        "    in_dim = size[0]\n",
        "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
        "    return tf.random.normal(shape=size, stddev=xavier_stddev)\n",
        "\n",
        "\n",
        "\"\"\" Q(z|X) \"\"\"\n",
        "X = tf.compat.v1.placeholder(tf.float32, shape=[None, X_dim])\n",
        "z = tf.compat.v1.placeholder(tf.float32, shape=[None, z_dim])\n",
        "\n",
        "Q_W1 = tf.Variable(xavier_init([X_dim, h_dim]))\n",
        "Q_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n",
        "\n",
        "Q_W2 = tf.Variable(xavier_init([h_dim, z_dim]))\n",
        "Q_b2 = tf.Variable(tf.zeros(shape=[z_dim]))\n",
        "\n",
        "theta_Q = [Q_W1, Q_W2, Q_b1, Q_b2]\n",
        "\n",
        "\n",
        "def Q(X):\n",
        "    h = tf.nn.relu(tf.matmul(X, Q_W1) + Q_b1)\n",
        "    z = tf.matmul(h, Q_W2) + Q_b2\n",
        "    return z\n",
        "\n",
        "\n",
        "\"\"\" P(X|z) \"\"\"\n",
        "P_W1 = tf.Variable(xavier_init([z_dim, h_dim]))\n",
        "P_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n",
        "\n",
        "P_W2 = tf.Variable(xavier_init([h_dim, X_dim]))\n",
        "P_b2 = tf.Variable(tf.zeros(shape=[X_dim]))\n",
        "\n",
        "theta_P = [P_W1, P_W2, P_b1, P_b2]\n",
        "\n",
        "\n",
        "def P(z):\n",
        "    h = tf.nn.relu(tf.matmul(z, P_W1) + P_b1)\n",
        "    logits = tf.matmul(h, P_W2) + P_b2\n",
        "    prob = tf.nn.sigmoid(logits)\n",
        "    return prob, logits\n",
        "\n",
        "\n",
        "\"\"\" D(z) \"\"\"\n",
        "D_W1 = tf.Variable(xavier_init([z_dim, h_dim]))\n",
        "D_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n",
        "\n",
        "D_W2 = tf.Variable(xavier_init([h_dim, 1]))\n",
        "D_b2 = tf.Variable(tf.zeros(shape=[1]))\n",
        "\n",
        "theta_D = [D_W1, D_W2, D_b1, D_b2]\n",
        "\n",
        "\n",
        "def D(z):\n",
        "    h = tf.nn.relu(tf.matmul(z, D_W1) + D_b1)\n",
        "    logits = tf.matmul(h, D_W2) + D_b2\n",
        "    prob = tf.nn.sigmoid(logits)\n",
        "    return prob\n",
        "\n",
        "\n",
        "\"\"\" Training \"\"\"\n",
        "z_sample = Q(X)\n",
        "_, logits = P(z_sample)\n",
        "\n",
        "# Sample from random z\n",
        "X_samples, _ = P(z)\n",
        "\n",
        "# E[log P(X|z)]\n",
        "recon_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=X))\n",
        "\n",
        "# Adversarial loss to approx. Q(z|X)\n",
        "D_real = D(z)\n",
        "D_fake = D(z_sample)\n",
        "\n",
        "D_loss = -tf.reduce_mean(tf.tf.math.log(D_real) + tf.tf.math.log(1. - D_fake))\n",
        "G_loss = -tf.reduce_mean(tf.tf.math.log(D_fake))\n",
        "\n",
        "AE_solver = tf.train.AdamOptimizer().minimize(recon_loss, var_list=theta_P + theta_Q)\n",
        "D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)\n",
        "G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_Q)\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "if not os.path.exists('out/'):\n",
        "    os.makedirs('out/')\n",
        "\n",
        "i = 0\n",
        "\n",
        "for it in range(1000000):\n",
        "    X_mb, _ = mnist.train.next_batch(mb_size)\n",
        "    z_mb = np.random.randn(mb_size, z_dim)\n",
        "\n",
        "    _, recon_loss_curr = sess.run([AE_solver, recon_loss], feed_dict={X: X_mb})\n",
        "    _, D_loss_curr = sess.run([D_solver, D_loss], feed_dict={X: X_mb, z: z_mb})\n",
        "    _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict={X: X_mb})\n",
        "\n",
        "    if it % 1000 == 0:\n",
        "        print('Iter: {}; D_loss: {:.4}; G_loss: {:.4}; Recon_loss: {:.4}'\n",
        "              .format(it, D_loss_curr, G_loss_curr, recon_loss_curr))\n",
        "\n",
        "        samples = sess.run(X_samples, feed_dict={z: np.random.randn(16, z_dim)})\n",
        "\n",
        "        fig = plot(samples)\n",
        "        plt.savefig('out/{}.png'.format(str(i).zfill(3)), bbox_inches='tight')\n",
        "        i += 1\n",
        "        plt.close(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwYiAYdldXRp"
      },
      "outputs": [],
      "source": [
        "#Net 1- initialise\n",
        "def train_500(input):\n",
        "  W_500=[]\n",
        "  with tf.Graph().as_default():\n",
        "    rbm = train_cdk(1,784,650,.001,32)\n",
        "  for i in range(500,input.shape[0]+500,500):\n",
        "    print(f'Iteration{i/500}')\n",
        "    trX_sampled=input[(i-500):i]\n",
        "    w=rbm.ex_w(trX_sampled)\n",
        "    W=np.array(w[0])\n",
        "    pca_w=compress(W)\n",
        "    W_500.append(pca_w)\n",
        "  W=np.array(W_500)\n",
        "  print(W.shape)\n",
        "  return W"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYPXooTy39lx",
        "outputId": "991b65bb-4b78-462c-ac17-d02a5eff016b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration1.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6935.02685546875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6935.0166015625:\n",
            "Iteration2.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7183.2490234375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7183.24853515625:\n",
            "Iteration3.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7593.73193359375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7593.7255859375:\n",
            "Iteration4.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7058.8056640625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7058.8017578125:\n",
            "Iteration5.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7351.345703125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7351.3359375:\n",
            "Iteration6.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7301.1123046875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7301.103515625:\n",
            "Iteration7.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6624.81884765625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6624.81494140625:\n",
            "Iteration8.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7139.03857421875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7139.03955078125:\n",
            "Iteration9.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7010.94873046875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7010.94091796875:\n",
            "Iteration10.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7673.21337890625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7673.21337890625:\n",
            "Iteration11.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7841.84033203125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7841.83642578125:\n",
            "Iteration12.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7471.2568359375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7471.25927734375:\n",
            "Iteration13.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 8205.3447265625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 8205.341796875:\n",
            "Iteration14.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6761.8076171875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6761.79833984375:\n",
            "Iteration15.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6180.80517578125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6180.80126953125:\n",
            "Iteration16.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7234.14453125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7234.14013671875:\n",
            "Iteration17.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7470.48876953125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7470.482421875:\n",
            "Iteration18.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7802.29833984375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7802.287109375:\n",
            "Iteration19.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6961.81005859375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6961.80419921875:\n",
            "Iteration20.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7167.4931640625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7167.4833984375:\n",
            "Iteration21.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 8246.3154296875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 8246.30859375:\n",
            "Iteration22.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7377.8154296875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7377.80615234375:\n",
            "Iteration23.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7361.35400390625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7361.34130859375:\n",
            "Iteration24.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7141.9365234375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7141.92578125:\n",
            "Iteration25.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7753.03759765625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7753.037109375:\n",
            "Iteration26.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7410.06884765625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7410.06494140625:\n",
            "Iteration27.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 8058.41943359375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 8058.4111328125:\n",
            "Iteration28.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7048.30029296875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7048.29833984375:\n",
            "Iteration29.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7939.02978515625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7939.0185546875:\n",
            "Iteration30.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6896.7392578125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6896.73388671875:\n",
            "Iteration31.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6537.13232421875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6537.12060546875:\n",
            "Iteration32.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6950.33447265625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6950.32861328125:\n",
            "Iteration33.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7425.70361328125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7425.69921875:\n",
            "Iteration34.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6767.39404296875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6767.388671875:\n",
            "Iteration35.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7266.95703125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7266.95068359375:\n",
            "Iteration36.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6504.76025390625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6504.76123046875:\n",
            "Iteration37.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6951.6806640625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6951.6748046875:\n",
            "Iteration38.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6944.22412109375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6944.21533203125:\n",
            "Iteration39.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7571.17724609375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7571.17529296875:\n",
            "Iteration40.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 8108.88427734375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 8108.8759765625:\n",
            "Iteration41.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7318.12841796875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7318.12451171875:\n",
            "Iteration42.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7451.8369140625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7451.830078125:\n",
            "Iteration43.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7128.0302734375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7128.025390625:\n",
            "Iteration44.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7582.68212890625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7582.67822265625:\n",
            "Iteration45.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7098.62158203125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7098.61865234375:\n",
            "Iteration46.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 8061.3564453125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 8061.35205078125:\n",
            "Iteration47.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7823.7177734375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7823.70751953125:\n",
            "Iteration48.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6904.6875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6904.6806640625:\n",
            "Iteration49.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7314.32470703125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7314.31689453125:\n",
            "Iteration50.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7082.150390625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7082.140625:\n",
            "Iteration51.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7735.09033203125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7735.0810546875:\n",
            "Iteration52.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6738.6435546875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6738.63818359375:\n",
            "Iteration53.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6894.14599609375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6894.1376953125:\n",
            "Iteration54.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6972.3662109375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6972.361328125:\n",
            "Iteration55.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7512.8701171875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7512.86474609375:\n",
            "Iteration56.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6864.98828125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6864.986328125:\n",
            "Iteration57.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 8157.3212890625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 8157.31201171875:\n",
            "Iteration58.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7587.15869140625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7587.154296875:\n",
            "Iteration59.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7571.89794921875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7571.89697265625:\n",
            "Iteration60.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6360.28271484375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6360.2734375:\n",
            "Iteration61.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7114.72998046875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7114.716796875:\n",
            "Iteration62.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6437.09912109375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6437.091796875:\n",
            "Iteration63.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6635.4482421875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6635.4404296875:\n",
            "Iteration64.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6507.3671875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6507.36181640625:\n",
            "Iteration65.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7770.685546875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7770.6796875:\n",
            "Iteration66.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6847.7099609375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6847.69970703125:\n",
            "Iteration67.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7348.6015625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7348.5986328125:\n",
            "Iteration68.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6940.88037109375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6940.87646484375:\n",
            "Iteration69.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7085.55419921875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7085.55078125:\n",
            "Iteration70.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7382.123046875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7382.11962890625:\n",
            "Iteration71.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7200.2392578125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7200.23779296875:\n",
            "Iteration72.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6762.708984375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6762.7021484375:\n",
            "Iteration73.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7443.59423828125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7443.58740234375:\n",
            "Iteration74.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 8186.31787109375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 8186.31005859375:\n",
            "Iteration75.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7843.35693359375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7843.3515625:\n",
            "Iteration76.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7053.42431640625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7053.42041015625:\n",
            "Iteration77.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6927.54931640625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6927.5458984375:\n",
            "Iteration78.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7212.025390625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7212.02197265625:\n",
            "Iteration79.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7794.01806640625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7794.01123046875:\n",
            "Iteration80.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6902.88525390625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6902.8828125:\n",
            "Iteration81.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7340.7939453125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7340.7939453125:\n",
            "Iteration82.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6987.9345703125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6987.9267578125:\n",
            "Iteration83.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7045.66748046875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7045.65673828125:\n",
            "Iteration84.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6725.78662109375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6725.7861328125:\n",
            "Iteration85.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7455.25927734375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7455.25146484375:\n",
            "Iteration86.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7519.33642578125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7519.3271484375:\n",
            "Iteration87.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7118.6435546875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7118.6376953125:\n",
            "Iteration88.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6911.0556640625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6911.04736328125:\n",
            "Iteration89.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6939.01953125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6939.013671875:\n",
            "Iteration90.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6858.30419921875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6858.29736328125:\n",
            "Iteration91.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7025.69580078125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7025.69140625:\n",
            "Iteration92.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6720.974609375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6720.96923828125:\n",
            "Iteration93.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6637.5283203125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6637.52294921875:\n",
            "Iteration94.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7462.73388671875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7462.72509765625:\n",
            "Iteration95.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6897.0185546875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6897.0107421875:\n",
            "Iteration96.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7849.47705078125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7849.46923828125:\n",
            "Iteration97.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 8149.46923828125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 8149.46337890625:\n",
            "Iteration98.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7380.52978515625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7380.51904296875:\n",
            "Iteration99.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7101.86181640625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7101.8515625:\n",
            "Iteration100.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6839.95556640625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6839.9462890625:\n",
            "Iteration101.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6806.85205078125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6806.84521484375:\n",
            "Iteration102.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7194.16796875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7194.162109375:\n",
            "Iteration103.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7653.9365234375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7653.93310546875:\n",
            "Iteration104.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6893.38427734375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6893.38232421875:\n",
            "Iteration105.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6654.35546875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6654.3525390625:\n",
            "Iteration106.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7036.19140625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7036.185546875:\n",
            "Iteration107.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7182.978515625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7182.97265625:\n",
            "Iteration108.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6702.12939453125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6702.123046875:\n",
            "Iteration109.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7019.54736328125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7019.54931640625:\n",
            "Iteration110.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7472.14501953125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7472.13720703125:\n",
            "Iteration111.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7135.185546875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7135.17626953125:\n",
            "Iteration112.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7967.49560546875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7967.48779296875:\n",
            "Iteration113.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6890.38671875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6890.38037109375:\n",
            "Iteration114.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7232.81298828125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7232.80908203125:\n",
            "Iteration115.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6747.64013671875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6747.634765625:\n",
            "Iteration116.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6631.642578125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6631.6376953125:\n",
            "Iteration117.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7081.3837890625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7081.37451171875:\n",
            "Iteration118.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 6360.9990234375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 6360.99609375:\n",
            "Iteration119.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 8533.0078125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 8533.0009765625:\n",
            "Iteration120.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7096.216796875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7096.20654296875:\n",
            "(120, 784, 100)\n"
          ]
        }
      ],
      "source": [
        "w=train_500(trX_mnist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwKVtwMGj8yv"
      },
      "outputs": [],
      "source": [
        "##Net1\n",
        "def train_net1(n1_w,g=False):\n",
        "  n1_w=[]\n",
        "  final_weight=None\n",
        "  with tf.Graph().as_default():\n",
        "    rbm = train_cdk(1,784,650,.001,32)\n",
        "  w=n1_w.reshape(120,-1)\n",
        "  for i in range(10,w.shape[0]+10,10):\n",
        "    print(f\"Iteration {i/10}\")\n",
        "    temp=rbm.ex_w(w[i-10:i])\n",
        "    W=np.array(temp[0])\n",
        "    if (g==True) and (w.shape-i<=10):\n",
        "      return compress(W,1)\n",
        "    pca_w=compress(W)\n",
        "    n1_w.append(pca_w)\n",
        "  w_2=np.array(n1_w)\n",
        "  print(\"End of training of net 1\")\n",
        "  return w_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdXsDf3DWD5C"
      },
      "outputs": [],
      "source": [
        "def train_rbm(weights):\n",
        "  if 10**3>=weights.shape[0]>=10**2:\n",
        "    n1_w1=train_net1(weights,True)\n",
        "    return n1_w1\n",
        "  elif 10**4>=weights.shape[0]>=10**3:\n",
        "    n1_w1=train_net1(weights)\n",
        "    n1_w2=train_net1(n1_w1,True)\n",
        "    return n1_w2\n",
        "  elif 10**5>weights.shape[0]>=10**4:\n",
        "    n1_w1=train_net1(weights)\n",
        "    n1_w2=train_net1(n1_w1)\n",
        "    n1_w3=train_net1(n1_w2,True)\n",
        "    return n1_w3\n",
        "  elif weights.shape[0]>=10**5:\n",
        "    n1_w1=train_net1(weights)\n",
        "    n1_w2=train_net1(n1_w1)\n",
        "    n1_w3=train_net1(n1_w2)\n",
        "    n1_w4=train_net1(n1_w2,True)\n",
        "    return n1_w4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2skqLttJIon"
      },
      "outputs": [],
      "source": [
        "##Net 2\n",
        "class Net2:\n",
        "  def __init__(self,input,test,n):\n",
        "    self.train=input\n",
        "    self.test=test\n",
        "    self.batch_size=self.train.shape[0]\n",
        "    self.n2=n\n",
        "  def train_net2(self):\n",
        "    with tf.Graph().as_default():\n",
        "      rbm_1= train_cdk(1,784,650,.001,self.batch_size,self.n2)\n",
        "      #assign net 1 weights\n",
        "      784,1 or 784,650\n",
        "      rbm_1.as_w()\n",
        "      #extract weights after training on trX\n",
        "      temp_1=rbm_1.ex_w(self.train)\n",
        "      #reassign the previous weights\n",
        "      rbm_1.as_w()\n",
        "      W1=np.array(temp_1[0])\n",
        "      pca_w1,_=compress(W1)\n",
        "      \n",
        "      rbm_2= train_cdk(1,784,1,.001,self.batch_size,self.n2)\n",
        "      rbm_2.as_w()\n",
        "      #extract weights after training on compressed weights\n",
        "      temp_2=rbm_2.ex_w(np.transpose(pca_w1))\n",
        "      #reassign the previous weights\n",
        "      rbm_2.as_w()\n",
        "      W2=np.array(temp_2[0])\n",
        "      pca_w2,t2=compress(W2)\n",
        "      \n",
        "      rbm_3= train_cdk(1,784,650,.001,self.batch_size,self.n2)\n",
        "      rbm_3.as_w()\n",
        "      #extract weights after training on compressed weights\n",
        "      temp_3=rbm_3.ex_w(np.transpose(pca_w2))\n",
        "      #reassign the previous weights\n",
        "      rbm_3.as_w()\n",
        "      W3=np.array(temp_3[0])\n",
        "      pca_w3,t3=compress(W2)\n",
        "\n",
        "      rbm_4= train_cdk(1,784,650,.001,self.batch_size,self.n2)\n",
        "      rbm_4.as_w()\n",
        "      #extract weights after training on compressed weights\n",
        "      temp_4=rbm_4.ex_w(np.transpose(pca_w3))\n",
        "      #reassign the previous weights\n",
        "      rbm_4.as_w()\n",
        "      W4=np.array(temp_4[0])\n",
        "      pca_w4,t4=compress(W4)\n",
        "      \n",
        "      rbm_5= train_cdk(1,784,650,.001,self.batch_size,self.n2)\n",
        "      rbm_5.as_w()\n",
        "      #extract weights after training on compressed weights\n",
        "      temp_5=rbm_5.ex_w(np.transpose(pca_w4))\n",
        "      #reassign the previous weights\n",
        "      rbm_5.as_w()\n",
        "      W5=np.array(temp_5[0])\n",
        "      pca_w5,t5=compress(W5)\n",
        "      #update weights of previous rbm\n",
        "      rbm_4.update_weights(rbm_5.get_weights())\n",
        "      #reconstructing using rbm_4\n",
        "      re_4=rbm_4.train(np.transpose(t4),self.test)\n",
        "      #update weights of previous rbm\n",
        "      rbm_3.update_weights(re_4[0])\n",
        "      #reconstructing using rbm_3\n",
        "      re_3=rbm_3.train(np.transpose(t3),self.test)\n",
        "      #update weights of previous rbm\n",
        "      rbm_2.update_weights(re_3[0])\n",
        "      #reconstructing using rbm_2\n",
        "      re_2=rbm_2.train(np.transpose(t2),self.test)\n",
        "      #update weights of previous rbm\n",
        "      rbm_1.update_weights(re_2[0])\n",
        "      rbm_1.rbm.batch_size=32\n",
        "      re_1=rbm_1.train(self.train,self.test)\n",
        "      return re_1     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2sFyOq5mGl8"
      },
      "outputs": [],
      "source": [
        "def train_all(train,test):\n",
        "  n1_w=train_500(train,test)\n",
        "  n2=train_rbm(n1_w)\n",
        "  c=Net2(train,test,n2)\n",
        "  r=c.train_net2()\n",
        "  return r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_PpN9E5uo9U",
        "outputId": "936061a6-3139-4315-9043-42583401402e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration1.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6935.0244140625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6935.0166015625:\n",
            "Iteration2.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7183.25146484375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7183.24365234375:\n",
            "Iteration3.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7593.73388671875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7593.7255859375:\n",
            "Iteration4.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7058.80419921875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7058.7998046875:\n",
            "Iteration5.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7351.34423828125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7351.33984375:\n",
            "Iteration6.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7301.109375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7301.1025390625:\n",
            "Iteration7.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6624.818359375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6624.80908203125:\n",
            "Iteration8.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7139.04150390625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7139.03955078125:\n",
            "Iteration9.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7010.95166015625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7010.94775390625:\n",
            "Iteration10.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7673.2158203125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7673.2138671875:\n",
            "Iteration11.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7841.84033203125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7841.83251953125:\n",
            "Iteration12.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7471.26220703125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7471.2529296875:\n",
            "Iteration13.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 8205.3486328125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 8205.3427734375:\n",
            "Iteration14.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6761.80419921875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6761.80029296875:\n",
            "Iteration15.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6180.806640625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6180.7998046875:\n",
            "Iteration16.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7234.1455078125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7234.13623046875:\n",
            "Iteration17.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7470.486328125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7470.482421875:\n",
            "Iteration18.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7802.296875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7802.29150390625:\n",
            "Iteration19.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6961.81005859375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6961.79833984375:\n",
            "Iteration20.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7167.49267578125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7167.49267578125:\n",
            "Iteration21.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 8246.3134765625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 8246.3056640625:\n",
            "Iteration22.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7377.81640625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7377.8076171875:\n",
            "Iteration23.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7361.35009765625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7361.34619140625:\n",
            "Iteration24.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7141.93115234375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7141.9287109375:\n",
            "Iteration25.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7753.03466796875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7753.03369140625:\n",
            "Iteration26.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7410.0703125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7410.06494140625:\n",
            "Iteration27.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 8058.42041015625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 8058.4150390625:\n",
            "Iteration28.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7048.30419921875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7048.2939453125:\n",
            "Iteration29.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7939.02783203125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7939.0224609375:\n",
            "Iteration30.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6896.74072265625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6896.7333984375:\n",
            "Iteration31.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6537.1298828125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6537.12451171875:\n",
            "Iteration32.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6950.330078125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6950.33203125:\n",
            "Iteration33.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7425.70703125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7425.70263671875:\n",
            "Iteration34.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6767.39111328125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6767.392578125:\n",
            "Iteration35.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7266.95263671875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7266.94775390625:\n",
            "Iteration36.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6504.76123046875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6504.75732421875:\n",
            "Iteration37.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6951.68017578125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6951.67041015625:\n",
            "Iteration38.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6944.21875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6944.21875:\n",
            "Iteration39.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7571.18017578125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7571.169921875:\n",
            "Iteration40.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 8108.880859375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 8108.8740234375:\n",
            "Iteration41.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7318.13037109375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7318.12548828125:\n",
            "Iteration42.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7451.83740234375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7451.83203125:\n",
            "Iteration43.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7128.02490234375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7128.02880859375:\n",
            "Iteration44.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7582.6875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7582.67626953125:\n",
            "Iteration45.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7098.62158203125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7098.61376953125:\n",
            "Iteration46.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 8061.35498046875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 8061.3486328125:\n",
            "Iteration47.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7823.7138671875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7823.70263671875:\n",
            "Iteration48.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6904.69140625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6904.67919921875:\n",
            "Iteration49.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7314.32080078125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7314.31689453125:\n",
            "Iteration50.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7082.1494140625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7082.14208984375:\n",
            "Iteration51.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7735.08837890625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7735.08251953125:\n",
            "Iteration52.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6738.64501953125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6738.63818359375:\n",
            "Iteration53.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6894.14404296875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6894.138671875:\n",
            "Iteration54.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6972.369140625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6972.35986328125:\n",
            "Iteration55.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7512.875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7512.86669921875:\n",
            "Iteration56.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6864.986328125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6864.982421875:\n",
            "Iteration57.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 8157.31640625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 8157.30859375:\n",
            "Iteration58.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7587.15625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7587.15576171875:\n",
            "Iteration59.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7571.9013671875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7571.89111328125:\n",
            "Iteration60.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6360.28076171875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6360.27294921875:\n",
            "Iteration61.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7114.732421875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7114.71826171875:\n",
            "Iteration62.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6437.09912109375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6437.0927734375:\n",
            "Iteration63.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6635.4423828125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6635.4404296875:\n",
            "Iteration64.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6507.36572265625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6507.36328125:\n",
            "Iteration65.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7770.6865234375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7770.67578125:\n",
            "Iteration66.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6847.7041015625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6847.69580078125:\n",
            "Iteration67.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7348.6015625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7348.603515625:\n",
            "Iteration68.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6940.876953125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6940.87255859375:\n",
            "Iteration69.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7085.5576171875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7085.55224609375:\n",
            "Iteration70.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7382.12158203125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7382.11669921875:\n",
            "Iteration71.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7200.248046875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7200.2392578125:\n",
            "Iteration72.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6762.71142578125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6762.7021484375:\n",
            "Iteration73.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7443.59130859375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7443.5859375:\n",
            "Iteration74.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 8186.31494140625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 8186.310546875:\n",
            "Iteration75.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7843.359375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7843.35302734375:\n",
            "Iteration76.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7053.42578125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7053.4228515625:\n",
            "Iteration77.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6927.55078125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6927.546875:\n",
            "Iteration78.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7212.02099609375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7212.01220703125:\n",
            "Iteration79.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7794.0205078125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7794.01025390625:\n",
            "Iteration80.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6902.884765625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6902.87890625:\n",
            "Iteration81.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7340.7998046875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7340.7890625:\n",
            "Iteration82.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6987.9375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6987.9326171875:\n",
            "Iteration83.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7045.67236328125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7045.65966796875:\n",
            "Iteration84.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6725.7939453125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6725.78271484375:\n",
            "Iteration85.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7455.26123046875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7455.25537109375:\n",
            "Iteration86.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7519.3330078125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7519.3271484375:\n",
            "Iteration87.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7118.63916015625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7118.63818359375:\n",
            "Iteration88.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6911.05126953125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6911.05322265625:\n",
            "Iteration89.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6939.01708984375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6939.0107421875:\n",
            "Iteration90.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6858.302734375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6858.29736328125:\n",
            "Iteration91.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7025.69677734375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7025.69140625:\n",
            "Iteration92.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6720.97705078125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6720.96923828125:\n",
            "Iteration93.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6637.5302734375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6637.52099609375:\n",
            "Iteration94.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7462.73681640625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7462.73291015625:\n",
            "Iteration95.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6897.01953125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6897.01220703125:\n",
            "Iteration96.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7849.47607421875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7849.46875:\n",
            "Iteration97.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 8149.474609375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 8149.46533203125:\n",
            "Iteration98.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7380.52783203125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7380.5224609375:\n",
            "Iteration99.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7101.8603515625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7101.845703125:\n",
            "Iteration100.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6839.9541015625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6839.94580078125:\n",
            "Iteration101.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6806.85009765625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6806.84521484375:\n",
            "Iteration102.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7194.1669921875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7194.1650390625:\n",
            "Iteration103.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7653.93896484375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7653.9326171875:\n",
            "Iteration104.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6893.3916015625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6893.3798828125:\n",
            "Iteration105.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6654.359375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6654.34375:\n",
            "Iteration106.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7036.1875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7036.18408203125:\n",
            "Iteration107.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7182.97998046875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7182.96630859375:\n",
            "Iteration108.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6702.13232421875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6702.125:\n",
            "Iteration109.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7019.54736328125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7019.5419921875:\n",
            "Iteration110.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7472.140625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7472.138671875:\n",
            "Iteration111.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7135.1875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7135.18017578125:\n",
            "Iteration112.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7967.49169921875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7967.4814453125:\n",
            "Iteration113.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6890.3876953125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6890.3818359375:\n",
            "Iteration114.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7232.81640625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7232.80859375:\n",
            "Iteration115.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6747.64013671875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6747.63671875:\n",
            "Iteration116.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6631.64453125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6631.63623046875:\n",
            "Iteration117.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7081.37939453125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7081.37646484375:\n",
            "Iteration118.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 6361.0:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 6360.9951171875:\n",
            "Iteration119.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 8533.0078125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 8533.0009765625:\n",
            "Iteration120.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 7096.21533203125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 7096.20849609375:\n",
            "Iteration 1.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 1056.650634765625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 1056.41162109375:\n",
            "Iteration 2.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 1097.17724609375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 1096.4005126953125:\n",
            "Iteration 3.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 1134.505859375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 1133.66015625:\n",
            "Iteration 4.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 1047.0767822265625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 1047.3995361328125:\n",
            "Iteration 5.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 1100.1163330078125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 1099.9403076171875:\n",
            "Iteration 6.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 1080.454833984375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 1081.3052978515625:\n",
            "Iteration 7.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 1032.064208984375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 1031.704345703125:\n",
            "Iteration 8.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 1102.17578125:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 1102.404541015625:\n",
            "Iteration 9.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 1034.96435546875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 1034.2955322265625:\n",
            "Iteration 10.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 1050.9439697265625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 1050.2156982421875:\n",
            "Iteration 11.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 1041.71240234375:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 1042.60302734375:\n",
            "Iteration 12.0\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 1: Error: 1066.5115966796875:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 684: Epoch: 2: Error: 1066.1448974609375:\n",
            "Learning Rate: 0.001:  Batch Size: 60000:  Hidden Layers: 650: Epoch: 1: Error: 7246.29150390625:\n",
            "Learning Rate: 0.001:  Batch Size: 60000:  Hidden Layers: 650: Epoch: 2: Error: 7246.25732421875:\n",
            "Learning Rate: 0.001:  Batch Size: 60000:  Hidden Layers: 650: Epoch: 1: Error: 0.4928390383720398:\n",
            "Learning Rate: 0.001:  Batch Size: 60000:  Hidden Layers: 650: Epoch: 2: Error: 0.5057012438774109:\n",
            "Learning Rate: 0.001:  Batch Size: 60000:  Hidden Layers: 650: Epoch: 1: Error: 0.5040278434753418:\n",
            "Learning Rate: 0.001:  Batch Size: 60000:  Hidden Layers: 650: Epoch: 2: Error: 0.490177184343338:\n",
            "Learning Rate: 0.001:  Batch Size: 60000:  Hidden Layers: 650: Epoch: 1: Error: 0.5333740711212158:\n",
            "Learning Rate: 0.001:  Batch Size: 60000:  Hidden Layers: 650: Epoch: 2: Error: 0.4687722623348236:\n",
            "Learning Rate: 0.001:  Batch Size: 60000:  Hidden Layers: 650: Epoch: 1: Error: 0.5095036029815674:\n",
            "Learning Rate: 0.001:  Batch Size: 60000:  Hidden Layers: 650: Epoch: 2: Error: 0.48123180866241455:\n",
            "CD: 1\n",
            "Learning Rate: 0.001:  Batch Size: 60000:  Hidden Layers: 650: Epoch: 1: Error: 0.5020174384117126:\n",
            "Learning Rate: 0.001:  Batch Size: 60000:  Hidden Layers: 650: Epoch: 2: Error: 0.5009340643882751:\n",
            "CD: 1\n",
            "Learning Rate: 0.001:  Batch Size: 60000:  Hidden Layers: 650: Epoch: 1: Error: 0.5016340613365173:\n",
            "Learning Rate: 0.001:  Batch Size: 60000:  Hidden Layers: 650: Epoch: 2: Error: 0.5002503395080566:\n",
            "CD: 1\n",
            "Learning Rate: 0.001:  Batch Size: 60000:  Hidden Layers: 650: Epoch: 1: Error: 0.5008119940757751:\n",
            "Learning Rate: 0.001:  Batch Size: 60000:  Hidden Layers: 650: Epoch: 2: Error: 0.5016717314720154:\n",
            "CD: 1\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 1: Error: 7217.07666015625:\n",
            "Learning Rate: 0.001:  Batch Size: 32:  Hidden Layers: 650: Epoch: 2: Error: 7217.07666015625:\n"
          ]
        }
      ],
      "source": [
        "Final_output=train_all(trX_mnist,teX_mnist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7R1erYNwceN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "2vs_nFEXuXb1"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
